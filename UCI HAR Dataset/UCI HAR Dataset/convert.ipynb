{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "unzip:  cannot find or open human+activity+recognition+using+smartphones.zip, human+activity+recognition+using+smartphones.zip.zip or human+activity+recognition+using+smartphones.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the dataset: https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip\n",
    "\n",
    "unzip it to this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {1:\"WALKING\",\n",
    "2:\"WALKING_UPSTAIRS\",\n",
    "3:\"WALKING_DOWNSTAIRS\",\n",
    "4:\"SITTING\",\n",
    "5:\"STANDING\",\n",
    "6:\"LAYING\",}\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('test/Inertial Signals'):\n",
    "    if \"cleaned\" in filename:\n",
    "        continue\n",
    "    new = []\n",
    "    with open(f'test/Inertial Signals/{filename}','r') as fid:    \n",
    "        for line in fid.readlines():\n",
    "            new.append(line.strip().replace(\"  \",\" \").replace(\" \",\"\\t\"))\n",
    "            \n",
    "    with open(f'test/Inertial Signals/cleaned_{filename}','w') as fid:\n",
    "        for line in new:\n",
    "            fid.write(line+\"\\n\")\n",
    "        \n",
    "for filename in os.listdir('train/Inertial Signals'):\n",
    "    if \"cleaned\" in filename:\n",
    "        continue\n",
    "    new = []\n",
    "    with open(f'train/Inertial Signals/{filename}','r') as fid:    \n",
    "        for line in fid.readlines():\n",
    "            new.append(line.strip().replace(\"  \",\" \").replace(\" \",\"\\t\"))\n",
    "            \n",
    "    with open(f'train/Inertial Signals/cleaned_{filename}','w') as fid:\n",
    "        for line in new:\n",
    "            fid.write(line+\"\\n\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {}\n",
    "for filename in os.listdir('test/Inertial Signals'):\n",
    "    if \"cleaned\" not in filename:\n",
    "        continue\n",
    "    \n",
    "    test_dict[filename] = pd.read_csv(f'test/Inertial Signals/{filename}', sep='\\t', header=None).T\n",
    "    \n",
    "test_y = pd.read_csv(\"test/y_test.txt\", header=None, names=['label'])\n",
    "subject_df = pd.read_csv(\"test/subject_test.txt\",  header=None, names=['subject'])\n",
    "#test_y['Subject'] = subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "for filename in os.listdir('train/Inertial Signals'):\n",
    "    if \"cleaned\" not in filename:\n",
    "        continue\n",
    "    \n",
    "    train_dict[filename] = pd.read_csv(f'train/Inertial Signals/{filename}', sep='\\t', header=None).T\n",
    "    \n",
    "train_y = pd.read_csv(\"train/y_train.txt\", names=['label'], header=None)\n",
    "train_subject_df = pd.read_csv(\"train/subject_train.txt\",  header=None, names=['subject'])\n",
    "#test_y['Subject'] = subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cleaned_body_acc_x_train.txt', 'cleaned_body_acc_y_train.txt', 'cleaned_body_acc_z_train.txt', 'cleaned_body_gyro_x_train.txt', 'cleaned_body_gyro_y_train.txt', 'cleaned_body_gyro_z_train.txt', 'cleaned_total_acc_x_train.txt', 'cleaned_total_acc_y_train.txt', 'cleaned_total_acc_z_train.txt'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_data(data_dict, y_df, subject_df, set_type):\n",
    "    dai = []\n",
    "    count = defaultdict(int)\n",
    "    for i in range(y_df.shape[0]):\n",
    "        tmp_df=pd.DataFrame()\n",
    "        for key in data_dict.keys():\n",
    "            tmp_df[key.replace(\"cleaned_\",\"\").replace(\"_train.txt\",\"\").replace(\"_test.txt\",\"\")] = data_dict[key][i]\n",
    "        \n",
    "        label = class_map[y_df.loc[i][0]]    \n",
    "        subject = subject_df.loc[i][0]\n",
    "        count[f\"{subject}_{label}\"]+=1\n",
    "        index = count[f\"{subject}_{label}\"]\n",
    "        tmp_df.to_csv(f\"data/{subject}_{label}_{index:02d}.csv\", index=None)\n",
    "        \n",
    "        tmp_dai = {'file_name': f\"{subject}_{label}_{index:02d}.csv\",\n",
    "                    \"metadata\":[{'name':\"Subject\", \"value\":f\"{subject}\"},\n",
    "                                {\"name\": \"Set\", \"value\": set_type}],\n",
    "                                \"sessions\":[\n",
    "                                    {'session_name':\"Label Session\",\n",
    "                                    \"segments\":[{\"name\":\"Label\",\n",
    "                                                \"value\":label,\n",
    "                                                \"start\":0,\n",
    "                                                \"end\":127}]}\n",
    "                                ]}\n",
    "        \n",
    "        dai.append(tmp_dai)\n",
    "        \n",
    "    return dai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_data_combined(data_dict, metadata_df, set_type):\n",
    "    dai = []\n",
    "    count = defaultdict(int)\n",
    "    groups = metadata_df.groupby(['label','subject'])\n",
    "    for key in groups.groups.keys():\n",
    "        M = []\n",
    "        segments = []\n",
    "        label = class_map[key[0]]    \n",
    "        count = 0\n",
    "        subject = key[1]        \n",
    "        for i in groups.get_group(key).index.values:\n",
    "\n",
    "            tmp_df=pd.DataFrame()\n",
    "            #for key in data_dict.keys():\n",
    "            #    tmp_df[key.replace(\"cleaned_\",\"\").replace(\"_train.txt\",\"\").replace(\"_test.txt\",\"\")] = data_dict[key][i]\n",
    "            \n",
    "            segments.append({\"name\":\"Label\",\"value\":label,\"start\":count*128,\"end\":(count+1)*128-1})                      \n",
    "            #M.append(tmp_df)\n",
    "            count+=1\n",
    "        \n",
    "            \n",
    "        \n",
    "        tmp_dai = {'file_name': f\"{subject}_{label}.csv\",\n",
    "                        \"metadata\":[{'name':\"Subject\", \"value\":f\"{subject}\"},\n",
    "                                    {\"name\": \"Set\", \"value\": set_type}],\n",
    "                                    \"sessions\":[\n",
    "                                        {'session_name':\"Label Session\",\n",
    "                                        \"segments\":segments}\n",
    "                                    ]}    \n",
    "        dai.append(tmp_dai)  \n",
    "        #capture_df = pd.concat(M).reset_index(drop=True)\n",
    "        #capture_df.to_csv(f\"data/{subject}_{label}.csv\", index=None)\n",
    "        \n",
    "    return dai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y['subject'] = subject_df\n",
    "train_y['subject']= train_subject_df\n",
    "\n",
    "dai_test = convert_data_combined(test_dict, test_y, \"Test\")\n",
    "dai_train = convert_data_combined(train_dict, train_y, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.dump(dai_test+dai_train, open(\"data/project.dai\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dai_test = convert_data(test_dict, test_y, subject_df)\n",
    "dai_train = convert_data(train_dict, train_y, train_subject_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(dai_test+dai_train, open(\"data/project.dai\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
